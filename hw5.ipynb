{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFI9t+7/jJZIl90KXmQuhk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hsuan7/Deep-generative-models/blob/main/hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-_ABKKRfgkf"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers peft bitsandbytes scikit-learn seaborn matplotlib torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall bitsandbytes\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "06wVNTSVGXQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import re # 用於解析\n",
        "\n",
        "# 抑制不必要的警告\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# --- 參數設定 ---\n",
        "# 使用的模型：Qwen、Gemma\n",
        "# GEMMA_MODEL_NAME = \"google/gemma-2-2b\" # For fine-tuning (Sequence Classification)\n",
        "# GEMMA_CHAT_MODEL_NAME = \"google/gemma-2-2b-it\" # For zero/few-shot inference\n",
        "\n",
        "LLAMA_MODEL_NAME = \"meta-llama/Llama-3.2-3B\"          # For fine-tuning (Sequence Classification)\n",
        "LLAMA_CHAT_MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"  # For zero/few-shot inference\n",
        "\n",
        "NUM_LABELS = 3\n",
        "LABELS = [\"low_risk\", \"mid_risk\", \"high_risk\"]\n",
        "ID2LABEL = {0: \"low_risk\", 1: \"mid_risk\", 2: \"high_risk\"}\n",
        "LABEL2ID = {\"low_risk\": 0, \"mid_risk\": 1, \"high_risk\": 2}\n",
        "TRAIN_EPOCHS = 1 # 保持 1 個 epoch 以便快速執行\n",
        "\n",
        "# 檢查 GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Warning: GPU not available. QLoRA fine-tuning requires a GPU.\")"
      ],
      "metadata": {
        "id": "fqPLcuwkGXOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. 資料預處理 ---\n",
        "print(\"\\n--- Step 1: Preprocessing ---\")\n",
        "\n",
        "# 載入資料集\n",
        "dataset = load_dataset(\"dair-ai/emotion\")\n",
        "print(\"Dataset loaded:\")\n",
        "print(dataset)\n",
        "\n",
        "# 定義風險映射函式\n",
        "def map_emotion_to_risk(example):\n",
        "    emotion = example['label']\n",
        "    if emotion in [1, 2, 5]:  # joy, love, surprise\n",
        "        example['risk_label'] = 0  # low_risk\n",
        "    elif emotion in [3, 4]:  # anger, fear\n",
        "        example['risk_label'] = 1  # mid_risk\n",
        "    elif emotion == 0:  # sadness\n",
        "        example['risk_label'] = 2  # high_risk\n",
        "    else:\n",
        "        example['risk_label'] = -1\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(map_emotion_to_risk)\n",
        "dataset = dataset.filter(lambda x: x['risk_label'] != -1)\n",
        "dataset = dataset.rename_column(\"risk_label\", \"labels\")\n",
        "\n",
        "print(\"Dataset after risk mapping:\")\n",
        "print(dataset['train'][0])\n",
        "\n",
        "# 載入 Tokenizer (用於 QLoRA) - 使用 Gemma 模型名稱\n",
        "# tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "# 2) 載入模型\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    LLAMA_MODEL_NAME,\n",
        "    num_labels=3,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",              # 或自行指定\n",
        "    # quantization_config=bnb_config # 若用 4-bit，這裡放你的 BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "# 3) 告訴模型 pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Tokenize 函式\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=False, max_length=512)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"label\"])\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "# 準備完整的測試集 (用於所有評估)\n",
        "test_dataset_full_tokenized = tokenized_datasets[\"test\"]\n",
        "original_test_set_full = dataset['test'] # 包含原始 text\n",
        "y_true_test_full = original_test_set_full['labels'] # 統一的真實標籤\n",
        "\n",
        "print(f\"Full test set size: {len(original_test_set_full)}\")\n",
        "\n",
        "# --- 評估用的輔助函式 ---\n",
        "def evaluate_model(y_true, y_pred, y_probs, title_prefix=\"\"):\n",
        "    print(f\"\\n--- {title_prefix} Evaluation ---\")\n",
        "\n",
        "    # F1 Score\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"F1 Score (Weighted): {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    try:\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # 繪製 CM\n",
        "        fig, ax = plt.subplots()\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=LABELS)\n",
        "        disp.plot(ax=ax, cmap='Blues')\n",
        "        plt.title(f\"{title_prefix} Confusion Matrix\") # 更新標題\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        cm_filename = f\"plot_cm_{title_prefix.lower().replace(' ', '_').replace('-', '_')}.png\" # 更新檔案名稱\n",
        "        plt.savefig(cm_filename)\n",
        "        print(f\"Saved confusion matrix to {cm_filename}\")\n",
        "        plt.close(fig)\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting confusion matrix: {e}\")\n",
        "\n",
        "    # 需要機率的指標\n",
        "    if y_probs is not None and len(np.unique(y_true)) > 1:\n",
        "        try:\n",
        "            # 標籤二元化\n",
        "            y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n",
        "\n",
        "            if y_true_bin.shape[1] == 3 and y_probs.shape[1] != 3:\n",
        "                print(f\"Warning: Probability shape mismatch. Adjusting.\")\n",
        "                temp_probs = np.zeros((y_probs.shape[0], 3))\n",
        "                if y_probs.shape[1] < 3:\n",
        "                     temp_probs[:, :y_probs.shape[1]] = y_probs\n",
        "                y_probs = temp_probs\n",
        "\n",
        "            # AUROC\n",
        "            auroc = roc_auc_score(y_true_bin, y_probs, average='weighted', multi_class='ovr')\n",
        "            print(f\"AUROC (Weighted, OVR): {auroc:.4f}\")\n",
        "\n",
        "            # PR-AUC\n",
        "            pr_auc = average_precision_score(y_true_bin, y_probs, average='weighted')\n",
        "            print(f\"PR-AUC (Weighted): {pr_auc:.4f}\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Could not calculate AUROC/PR-AUC: {e}\")\n",
        "    else:\n",
        "        print(\"Skipping AUROC/PR-AUC (no probabilities or only one class present).\")\n",
        "\n",
        "\n",
        "# --- 共用的輔助函式 ---\n",
        "def parse_response(response):\n",
        "    \"\"\"解析模型的生成式回應\"\"\"\n",
        "    response = response.lower()\n",
        "    if \"high_risk\" in response or \"(2)\" in response:\n",
        "        return 2\n",
        "    elif \"mid_risk\" in response or \"(1)\" in response:\n",
        "        return 1\n",
        "    elif \"low_risk\" in response or \"(0)\" in response:\n",
        "        return 0\n",
        "    else:\n",
        "        # 嘗試只找數字\n",
        "        match = re.search(r'\\b([012])\\b', response)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "        else:\n",
        "            return 0 # 解析失敗時，預設為 low_risk"
      ],
      "metadata": {
        "id": "f2tvqnvTGXLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Zero-shot 推論 (使用 Gemma-Chat) ---\n",
        "# print(\"\\n--- Step 2: Zero-shot Inference (Gemma-Chat, Full 2000) ---\") # 更新標題\n",
        "print(\"\\n--- Step 2: Zero-shot Inference (LLAMA-Chat, Full 2000) ---\") # 更新標題\n",
        "print(\"WARNING: This step will run on all 2,000 test samples and will be EXTREMELY SLOW (potentially hours).\")\n",
        "\n",
        "def create_zero_shot_prompt(new_text):\n",
        "    \"\"\"建立 Zero-Shot (無範例) 的 Prompt\"\"\"\n",
        "    prompt = \"This is a text classification task. Classify the text into one of three risk categories: low_risk (0), mid_risk (1), or high_risk (2).\\n\\n\"\n",
        "    prompt += \"===\\n\\n\"\n",
        "    prompt += f\"Text: {new_text}\\nRisk:\"\n",
        "    return prompt\n",
        "\n",
        "try:\n",
        "    # 載入 Gemma-Chat 模型\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(LLAMA_CHAT_MODEL_NAME, trust_remote_code=True) # 使用 Gemma Chat 模型名稱\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLAMA_CHAT_MODEL_NAME, # 使用 Gemma Chat 模型名稱\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    if chat_tokenizer.pad_token is None:\n",
        "        chat_tokenizer.pad_token = chat_tokenizer.eos_token\n",
        "\n",
        "    print(f\"Zero-shot model loaded on device: {chat_model.device}\")\n",
        "\n",
        "    y_pred_zero_shot = []\n",
        "\n",
        "    # 迭代完整的 2000 筆資料\n",
        "    for i, test_sample in enumerate(original_test_set_full):\n",
        "        if (i+1) % 50 == 0 or i == 0:\n",
        "            print(f\"Running Zero-shot sample {i+1}/{len(original_test_set_full)}...\")\n",
        "\n",
        "        prompt_text = create_zero_shot_prompt(test_sample['text'])\n",
        "\n",
        "        # 格式化為 Chat (Gemma 的 Chat 格式)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt_text}\n",
        "        ]\n",
        "        text = chat_tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        model_inputs = chat_tokenizer([text], return_tensors=\"pt\").to(chat_model.device)\n",
        "\n",
        "        generated_ids = chat_model.generate(\n",
        "            model_inputs.input_ids,\n",
        "            max_new_tokens=10 # 只需要標籤\n",
        "        )\n",
        "        # Gemma 的 generate 會包含 prompt，需要移除\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        response = chat_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        y_pred_zero_shot.append(parse_response(response))\n",
        "\n",
        "    # 評估 (無機率)\n",
        "    evaluate_model(y_true_test_full, y_pred_zero_shot, y_probs=None, title_prefix=\"Zero-Shot (Gemma-Chat Full)\") # 更新標題\n",
        "\n",
        "    # 清理 VRAM，為下一步做準備\n",
        "    del chat_model\n",
        "    del chat_tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Zero-shot inference: {e}. Skipping.\")\n",
        "    # 確保清理\n",
        "    if 'chat_model' in locals(): del chat_model\n",
        "    if 'chat_tokenizer' in locals(): del chat_tokenizer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XtpqJQ6yGXJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Few-shot 推論 (使用 LLAMA-Chat, 平衡範例) ---\n",
        "print(\"\\n--- Step 3: Few-shot Inference (LLAMA-Chat, Full 2000) ---\") # 更新標題\n",
        "print(\"INFO: Using balanced 6-shot examples (one from each emotion).\")\n",
        "print(\"WARNING: This step will also be EXTREMELY SLOW (potentially hours).\")\n",
        "\n",
        "# 需要 concatenate_datasets 來組合\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "# 我們使用原始的文字資料集\n",
        "original_train_set = dataset['train']\n",
        "\n",
        "# 「平衡抽樣」\n",
        "# 原始標籤: 0: sadness, 1: joy, 2: love, 3: anger, 4: fear, 5: surprise\n",
        "print(\"Selecting 6 balanced few-shot examples...\")\n",
        "example_list = []\n",
        "for i in range(6):\n",
        "    # 從原始標籤 (label) 過濾，確保每種情緒都有\n",
        "    example_list.append(\n",
        "        original_train_set.filter(lambda x: x['label'] == i).select(range(1))\n",
        "    )\n",
        "\n",
        "# 將 6 個範例組合起來\n",
        "few_shot_examples = concatenate_datasets(example_list)\n",
        "print(\"Balanced examples selected:\")\n",
        "print(few_shot_examples['text'])\n",
        "print(few_shot_examples['labels']) # 應該會顯示 [2, 0, 0, 1, 1, 0] (對應的 risk)\n",
        "\n",
        "def create_few_shot_prompt(examples, new_text):\n",
        "    \"\"\"建立 Few-Shot (有範例) 的 Prompt\"\"\"\n",
        "    prompt = \"This is a text classification task. Classify the text into one of three risk categories: low_risk (0), mid_risk (1), or high_risk (2).\\n\\n\"\n",
        "    # 現在 'examples' 會有 6 筆\n",
        "    for ex in examples:\n",
        "        prompt += f\"Text: {ex['text']}\\nRisk: {ID2LABEL[ex['labels']]} ({ex['labels']})\\n\\n\"\n",
        "    prompt += \"===\\n\\n\"\n",
        "    prompt += f\"Text: {new_text}\\nRisk:\"\n",
        "    return prompt\n",
        "\n",
        "try:\n",
        "    # 載入 Gemma-Chat 模型\n",
        "    chat_tokenizer = AutoTokenizer.from_pretrained(LLAMA_CHAT_MODEL_NAME, trust_remote_code=True) # 使用 Gemma Chat 模型名稱\n",
        "    chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLAMA_CHAT_MODEL_NAME, # 使用 Gemma Chat 模型名稱\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    if chat_tokenizer.pad_token is None:\n",
        "        chat_tokenizer.pad_token = chat_tokenizer.eos_token\n",
        "\n",
        "    print(f\"Few-shot model loaded on device: {chat_model.device}\")\n",
        "\n",
        "    y_pred_few_shot = []\n",
        "\n",
        "    # 迭代完整的 2000 筆資料\n",
        "    for i, test_sample in enumerate(original_test_set_full):\n",
        "        if (i+1) % 50 == 0 or i == 0:\n",
        "            print(f\"Running Few-shot sample {i+1}/{len(original_test_set_full)}...\")\n",
        "\n",
        "        prompt_text = create_few_shot_prompt(few_shot_examples, test_sample['text'])\n",
        "\n",
        "        # 格式化為 Chat (Gemma 的 Chat 格式)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt_text}\n",
        "        ]\n",
        "        text = chat_tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        model_inputs = chat_tokenizer([text], return_tensors=\"pt\").to(chat_model.device)\n",
        "\n",
        "        generated_ids = chat_model.generate(\n",
        "            model_inputs.input_ids,\n",
        "            max_new_tokens=10 # 只需要標籤\n",
        "        )\n",
        "        # Gemma 的 generate 會包含 prompt，需要移除\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        response = chat_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        y_pred_few_shot.append(parse_response(response))\n",
        "\n",
        "    # 評估 (無機率)\n",
        "    evaluate_model(y_true_test_full, y_pred_few_shot, y_probs=None, title_prefix=\"Few-Shot (LLAMA-Chat Full 6-Shot Balanced)\") # 更新標題\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during Few-shot inference: {e}. Skipping.\")\n",
        "finally:\n",
        "    # 清理 VRAM\n",
        "    if 'chat_model' in locals(): del chat_model\n",
        "    if 'chat_tokenizer' in locals(): del chat_tokenizer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "TbLcV-1eGXG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. LoRA / QLoRA Fine-tuning (使用 Gemma-Base) ---\n",
        "print(\"\\n--- Step 4: LoRA / QLoRA Fine-tuning (Gemma-Base) ---\") # 更新標題\n",
        "try:\n",
        "    # QLoRA 設定 (4-bit 量化)\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # 載入基底模型 (用於序列分類) - 使用 Gemma Base 模型名稱\n",
        "    qlora_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        LLAMA_MODEL_NAME, # 使用 Gemma Base 模型\n",
        "        num_labels=NUM_LABELS,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        id2label=ID2LABEL,\n",
        "        label2id=LABEL2ID,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    qlora_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # LoRA 設定\n",
        "    # lora_config = LoraConfig(\n",
        "    #     task_type=TaskType.SEQ_CLS,\n",
        "    #     r=16,\n",
        "    #     lora_alpha=32,\n",
        "    #     lora_dropout=0.1,\n",
        "    #     bias=\"none\",\n",
        "    #     # 鎖定 Gemma 的 attention 模組\n",
        "    #     target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"gemma_act\"], # 更新 target_modules\n",
        "    # )\n",
        "    lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"SEQ_CLS\"\n",
        "    )\n",
        "\n",
        "    # 套用 PEFT (LoRA)\n",
        "    peft_model = get_peft_model(qlora_model, lora_config)\n",
        "    peft_model.print_trainable_parameters()\n",
        "\n",
        "    # Data Collator\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    # 訓練參數\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gemma-lora-emotion-risk\", # 更新輸出目錄\n",
        "        learning_rate=2e-4, # LoRA 可用較高學習率\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=TRAIN_EPOCHS,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        logging_steps=100,\n",
        "        report_to=\"none\", # 關閉 wandb\n",
        "        bf16=True if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else False,\n",
        "    )\n",
        "\n",
        "    # 建立 Trainer\n",
        "    trainer = Trainer(\n",
        "        model=peft_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # --- 5. (隱含) & 6. 訓練與評估 ---\n",
        "    print(\"Starting QLoRA fine-tuning...\")\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning complete.\")\n",
        "\n",
        "    print(\"\\n--- Step 6: Evaluating QLoRA Model ---\") # 更新標題\n",
        "\n",
        "    # 在測試集上預測\n",
        "    predictions = trainer.predict(test_dataset_full_tokenized) # 使用 tokenized 測試集\n",
        "\n",
        "    # 處理預測結果\n",
        "    y_pred_lora = np.argmax(predictions.predictions, axis=1)\n",
        "    y_probs_lora_logits = torch.from_numpy(predictions.predictions)\n",
        "    y_probs_lora = torch.nn.functional.softmax(y_probs_lora_logits, dim=1).numpy()\n",
        "    y_true_lora = predictions.label_ids # 這會等於 y_true_test_full\n",
        "\n",
        "    # 評估\n",
        "    evaluate_model(y_true_lora, y_pred_lora, y_probs_lora, title_prefix=\"QLoRA Fine-Tune (Gemma-Base)\") # 更新標題\n",
        "\n",
        "\n",
        "    # --- 7. 視覺化呈現 (使用 QLoRA 模型的結果) ---\n",
        "    print(\"\\n--- Step 7: Visualizing QLoRA Results ---\") # 更新標題\n",
        "\n",
        "    # 取得 high_risk (label 2) 的機率\n",
        "    p_high_risk = y_probs_lora[:, 2]\n",
        "\n",
        "    # 圖 1: 高風險走勢圖\n",
        "    try:\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.plot(p_high_risk, alpha=0.7, label=\"P(high_risk)\")\n",
        "        plt.title(\"High Risk Probability (P(high_risk)) Trend (Gemma-Base QLoRA Model)\") # 更新標題\n",
        "        plt.xlabel(\"Test Sample Index\")\n",
        "        plt.ylabel(\"P(high_risk)\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.savefig(\"plot_high_risk_trend_gemma.png\") # 更新檔案名稱\n",
        "        print(\"Saved high-risk trend plot to plot_high_risk_trend_gemma.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting trend: {e}\")\n",
        "\n",
        "    # 圖 2.1: 高風險滾動平均 (Rolling Window)\n",
        "    try:\n",
        "        # 使用 pandas 進行滾動平均\n",
        "        rolling_avg = pd.Series(p_high_risk).rolling(window=50, min_periods=1).mean()\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.plot(rolling_avg, color='red')\n",
        "        plt.title(\"Rolling Average (Window=50) of P(high_risk) (Gemma-Base QLoRA Model)\") # 更新標題\n",
        "        plt.xlabel(\"Test Sample Index\")\n",
        "        plt.ylabel(\"Rolling Avg P(high_risk)\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.savefig(\"plot_high_risk_rolling_avg_gemma.png\") # 更新檔案名稱\n",
        "        print(\"Saved high-risk rolling average plot to plot_high_risk_rolling_avg_gemma.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting rolling average: {e}\")\n",
        "\n",
        "    # 圖 2.2: 高風險濃度熱圖 (1D Heatmap)\n",
        "    try:\n",
        "        plt.figure(figsize=(18, 2)) # 寬而短\n",
        "        sns.heatmap(\n",
        "            [p_high_risk],\n",
        "            cmap=\"rocket\",\n",
        "            cbar=True,\n",
        "            cbar_kws={'label': 'P(high_risk)', 'orientation': 'horizontal', 'pad': 0.3},\n",
        "            xticklabels=False,\n",
        "            yticklabels=False\n",
        "        )\n",
        "        plt.title(\"High Risk Probability Concentration (Gemma-Base QLoRA Model)\") # 更新標題\n",
        "        plt.xlabel(\"Test Sample Index\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"plot_high_risk_concentration_heatmap_gemma.png\") # 更新檔案名稱\n",
        "        print(\"Saved high-risk concentration heatmap to plot_high_risk_concentration_heatmap_gemma.png\")\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting heatmap: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during QLoRA fine-tuning or evaluation: {e}\")\n",
        "    print(\"This may be due to CUDA OOM or other resource constraints.\")\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")"
      ],
      "metadata": {
        "id": "NOXduM9sGXEe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}